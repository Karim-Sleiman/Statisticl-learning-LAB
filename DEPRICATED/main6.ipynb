{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno \n",
    "from matplotlib.pyplot import subplots\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from ISLP import confusion_table\n",
    "from statsmodels.stats.outliers_influence \\\n",
    "     import variance_inflation_factor as VIF\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize,\n",
    "                         poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1-\n",
    "train_data = pd.read_csv('E:/6th year/maia work/2. Italy - Cassino/3-Statistical Learning/Final Assignement/Data/training_data.csv' , sep=\";\")\n",
    "test_data = pd.read_csv('E:/6th year/maia work/2. Italy - Cassino/3-Statistical Learning/Final Assignement/Data/test_data_no_target.csv', sep=\";\")\n",
    "train_data.info\n",
    "\n",
    "# Filter columns that exceed the threshold\n",
    "missing_values_count=train_data.isnull().sum()\n",
    "columns_exceeding_threshold = missing_values_count[missing_values_count > 100]\n",
    "\n",
    "\n",
    "columns_to_drop = missing_values_count[missing_values_count > 100].index\n",
    "train_data_cleaned = train_data.drop(columns=columns_to_drop)\n",
    "test_data_cleaned = test_data.drop(columns=columns_to_drop)\n",
    "train_data_cleaned.drop('Group', axis=1, inplace=True)\n",
    "test_data_cleaned.drop('Group', axis=1, inplace=True)\n",
    "train_data_cleaned\n",
    "### 2-\n",
    "def convert_decimal_separator(df):\n",
    "    # Iterate through each column in the DataFrame\n",
    "    for column in df.columns:\n",
    "        # Replace commas with periods in the data\n",
    "        df[column] = df[column].astype(str).str.replace(',', '.')\n",
    "        \n",
    "        # Convert the column to numeric type\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "# Call the function to convert decimal separator in X\n",
    "convert_decimal_separator(train_data_cleaned)\n",
    "\n",
    "# Call the function to convert decimal separator in X\n",
    "convert_decimal_separator(test_data_cleaned)\n",
    "\n",
    "\n",
    "### 3- Null to knn\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Assuming train_data_cleaned is your DataFrame\n",
    "\n",
    "# Display missing values by column\n",
    "missing_values_train = train_data_cleaned.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Initialize the KNNImputer with the 'nan_euclidean' metric\n",
    "knn_imputer = KNNImputer(n_neighbors=5, metric='nan_euclidean')\n",
    "\n",
    "# Fit and transform the data using KNN imputation\n",
    "# Apply KNN imputation to the numeric columns (float64 or int64)\n",
    "numeric_columns = train_data_cleaned.select_dtypes(include=['float64'])\n",
    "imputed_data = knn_imputer.fit_transform(numeric_columns)\n",
    "\n",
    "# Replace the original numeric columns in the DataFrame with the imputed data\n",
    "train_data_cleaned[numeric_columns.columns] = imputed_data\n",
    "\n",
    "# Check which columns still have missing values (this should be zero)\n",
    "missing_values_after_fill = train_data_cleaned.isnull().sum().sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming train_data_cleaned is your DataFrame\n",
    "\n",
    "# Display missing values by column\n",
    "missing_values_test = test_data_cleaned.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Initialize the KNNImputer with the 'nan_euclidean' metric\n",
    "knn_imputer = KNNImputer(n_neighbors=5, metric='nan_euclidean')\n",
    "\n",
    "# Fit and transform the data using KNN imputation\n",
    "# Apply KNN imputation to the numeric columns (float64 or int64)\n",
    "numeric_columns = test_data_cleaned.select_dtypes(include=['float64'])\n",
    "imputed_data = knn_imputer.fit_transform(numeric_columns)\n",
    "\n",
    "# Replace the original numeric columns in the DataFrame with the imputed data\n",
    "test_data_cleaned[numeric_columns.columns] = imputed_data\n",
    "\n",
    "# Check which columns still have missing values (this should be zero)\n",
    "missing_values_after_fill_test = test_data_cleaned.isnull().sum().sum()\n",
    "\n",
    "\n",
    "missing_train_values_later = train_data_cleaned.isnull().sum()\n",
    "\n",
    "missing_test_values_later = test_data_cleaned.isnull().sum()\n",
    "\n",
    "### 4-\n",
    "target= train_data_cleaned['Perform']\n",
    "classes= train_data_cleaned['Class']\n",
    "train_data_cleaned.drop(['Class','Perform'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names of X_train_df:\n",
      "Index(['I1', 'I2', 'I3', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I19',\n",
      "       'I20', 'I25', 'I28', 'I29', 'I30', 'I31', 'I33', 'I34', 'I35', 'I36',\n",
      "       'I37', 'I38', 'I39', 'I40', 'I41', 'I42', 'I43', 'I47', 'I53', 'I54',\n",
      "       'I56', 'dI1', 'dI2', 'dI3', 'dI5', 'dI6', 'dI7', 'dI8', 'dI9', 'dI10',\n",
      "       'dI11', 'dI19', 'dI20', 'dI25', 'dI29', 'dI30', 'dI31', 'dI33', 'dI34',\n",
      "       'dI35', 'dI36', 'dI37', 'dI39', 'dI40', 'dI41', 'dI42', 'dI47', 'dI53',\n",
      "       'dI54', 'dI56'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.033464017027995396, tolerance: 0.010999377067444005\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04109914030524919, tolerance: 0.010999377067444005\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0428709163082317, tolerance: 0.010999377067444005\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04323186143594171, tolerance: 0.010999377067444005\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0432673916066193, tolerance: 0.010999377067444005\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04323021002878136, tolerance: 0.010999377067444005\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.042981724323425397, tolerance: 0.010999377067444005\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04303167678610009, tolerance: 0.010999377067444005\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04322589849859071, tolerance: 0.010999377067444005\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04297812530917611, tolerance: 0.010999377067444005\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.016280803073911443, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.017187807415083967, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0189086814788908, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020112286732782536, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020288095869588574, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020285165918679127, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020251241308670842, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02021325604619051, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020176118691608735, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020140693160655587, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020107095066336456, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.020075297688123328, tolerance: 0.011191484505943929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015939592667209013, tolerance: 0.010865481642661929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.016109661206726855, tolerance: 0.010865481642661929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.016057623817573585, tolerance: 0.010865481642661929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.011366690099222865, tolerance: 0.010865481642661929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.022199307163845106, tolerance: 0.010865481642661929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02477769042909017, tolerance: 0.010865481642661929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.02280907179564906, tolerance: 0.010865481642661929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.021336790415119822, tolerance: 0.010865481642661929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "e:\\mp-anaconda3\\envs\\hm\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.022127580434045058, tolerance: 0.010865481642661929\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important features selected by Lasso: Index(['I6', 'I9', 'I28', 'I37', 'I41', 'I43', 'I47', 'dI6', 'dI9', 'dI25',\n",
      "       'dI29', 'dI47'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Store column names before splitting and scaling\n",
    "original_columns = train_data_cleaned.columns\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_cleaned, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale your features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create DataFrames with scaled features\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=original_columns)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=original_columns)\n",
    "\n",
    "# Now you can access column names from X_train_df, X_test_df, and y_train\n",
    "print(\"Column names of X_train_df:\")\n",
    "print(X_train_df.columns)\n",
    "\n",
    "# Instantiate LassoCV model\n",
    "lasso_cv = LassoCV(cv=5)  # Use 5-fold cross-validation to find the optimal alpha\n",
    "\n",
    "# Fit the model\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract important features\n",
    "important_features_indices = np.where(lasso_cv.coef_ != 0)[0]\n",
    "important_features = train_data_cleaned.columns[important_features_indices]\n",
    "\n",
    "print(\"Important features selected by Lasso:\", important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_select=['I6', 'I9', 'I28', 'I37', 'I41', 'I43', 'I47', 'dI6', 'dI9', 'dI25',\n",
    "        'dI29', 'dI47']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Select the columns from train_data_cleaned\n",
    "new_data = train_data_cleaned[columns_to_select]\n",
    "new_data_test = test_data_cleaned[columns_to_select]\n",
    "# Set column names for new_data\n",
    "new_data.columns = columns_to_select\n",
    "\n",
    "# Set column names for new_data_test\n",
    "new_data_test.columns = columns_to_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)  # Change degree as needed\n",
    "new_data = poly.fit_transform(new_data)\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `test_cleaned_data` is the cleaned test dataset and `train_data_cleaned` was the cleaned training dataset.\n",
    "\n",
    "# Step 1: Transform the test data using PolynomialFeatures (same instance used for training data)\n",
    "X_test_interactions = poly.transform(new_data_test)\n",
    "\n",
    "# Step 2: Convert the interaction features to a DataFrame using the column names from the training data\n",
    "new_data_test = pd.DataFrame(X_test_interactions, columns=poly.get_feature_names_out(new_data_test.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "### just run\n",
    "\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(new_data,classes,test_size=0.2,random_state=42)\n",
    "\n",
    "norm='std'\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Choose normalization method\n",
    "norm = 'min-max'  # Or 'std' or 'none'\n",
    "\n",
    "\n",
    "\n",
    "# Apply Min-Max normalization\n",
    "if norm == 'min-max':\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Standard normalization\n",
    "elif norm == 'std':\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "# If none, keep the original data\n",
    "else:\n",
    "    sys.exit(\"ERROR: Dataset Normalization must be one of [min-max, std, none]\")\n",
    "\n",
    "# Apply normalization if required\n",
    "if norm != 'none':\n",
    "    X_train_scaled = scaler.fit_transform(data_train)\n",
    "    X_val_scaled = scaler.transform(data_test)\n",
    "    \n",
    "   \n",
    "else:\n",
    "    X_train_scaled = data_train\n",
    "    X_val_scaled = data_test\n",
    "\n",
    "\n",
    "# Features indices for plotting\n",
    "X_train = np.array(data_train)\n",
    "X_val = np.array(data_test)\n",
    "\n",
    "\n",
    "\n",
    "feature_idx_1 = 0\n",
    "feature_idx_2 = 1\n",
    "\n",
    "\n",
    "\n",
    "model_name='LogisticRegression'\n",
    "kernel_type='sigmoid'\n",
    "import sys\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define KNeighborsClassifier model\n",
    "if model_name == 'KNN':\n",
    "    # Define the KNN model\n",
    "    knn_model = KNeighborsClassifier()\n",
    "\n",
    "    # Define hyperparameter grid for KNN\n",
    "    param_grid = {'n_neighbors': range(1, 30)} # neighbors can be from 1 to 21\n",
    "\n",
    "    # Hyperparameter tuning using cross-validation\n",
    "    grid_search = GridSearchCV(knn_model, param_grid, cv=20, scoring='accuracy')\n",
    "    grid_search.fit(X_train_scaled, target_train)\n",
    "\n",
    "    # Evaluate the best combination of hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    # Create KNN model with best parameters\n",
    "    model = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'])\n",
    "    print(\"Model: {}\\n\".format(model_name)) \n",
    "\n",
    "# define LogisticRegression model\n",
    "elif model_name == 'LogisticRegression':\n",
    "    # define model\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    print(\"Model: {}\".format(model_name))\n",
    "\n",
    "# define DecisionTreeClassifier model\n",
    "elif model_name == 'DecisionTree':\n",
    "    # define model\n",
    "    model = DecisionTreeClassifier()\n",
    "    print(\"Model: {}\".format(model_name))\n",
    "\n",
    "# define SVM model\n",
    "elif model_name == 'SVM':\n",
    "    if kernel_type == 'linear':\n",
    "        model = SVC(kernel='linear', random_state=42)\n",
    "    elif kernel_type == 'poly':\n",
    "        model = SVC(kernel='poly', degree=3, random_state=42)\n",
    "    elif kernel_type == 'rbf':\n",
    "        model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "    elif kernel_type == 'sigmoid':\n",
    "        model = SVC(kernel='sigmoid', random_state=42)\n",
    "    else:\n",
    "        sys.exit(\"ERROR: wrong kernel type [linear, poly, rbf, sigmoid]\")\n",
    "    print(\"Model: {}\".format(model_name))\n",
    "\n",
    "# define RandomForestClassifier model\n",
    "elif model_name == 'RandomForest':\n",
    "    # Define model with a specific random_state for reproducibility\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    print(\"Model: {}\".format(model_name))\n",
    "\n",
    "else:\n",
    "    sys.exit(\"ERROR: Model Creation [KNN, LogisticRegression, DecisionTree, SVM, RandomForest]\")\n",
    "\n",
    "# Train the model\n",
    "# Perform self-augmentation using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_augmented, y_train_augmented = smote.fit_resample(X_train_scaled, target_train)\n",
    "\n",
    "#model.fit(X_train_poly, target_train)#########3\n",
    "model.fit(X_train_augmented, y_train_augmented)\n",
    "\n",
    "# Perform predictions on the test set\n",
    "\n",
    "#predictions_test = model.predict(X_test_poly)########33\n",
    "predictions_test = model.predict(X_val_scaled)\n",
    "\n",
    "\n",
    "def FEDCIS_accuracy(prediction, ground_truth):\n",
    "    res = np.zeros(len(prediction))\n",
    "    for i in range(len(prediction)):\n",
    "        p = prediction[i]\n",
    "        gt = ground_truth.values[i]\n",
    "        if p == gt:\n",
    "            res[i] = 0\n",
    "        elif p == 0 and (gt == -1 or gt == 1):\n",
    "            res[i] = 1\n",
    "        elif (p == 1 or p == -1) and gt == 0:\n",
    "            res[i] = 1\n",
    "        elif p == -1 and gt == 1:\n",
    "            res[i] = 2\n",
    "        elif p == 1 and gt == -1:\n",
    "            res[i] = 2\n",
    "    return np.mean(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.858125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEDCIS_accuracy(predictions_test,target_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
